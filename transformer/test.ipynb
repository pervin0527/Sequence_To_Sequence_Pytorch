{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from data import Multi30kDataset, download_multi30k, make_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/pervinco/Datasets\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 5e-9\n",
    "ADAM_EPS = 5e-9\n",
    "SCHEDULER_FACTOR = 0.9\n",
    "SCHEDULER_PATIENCE = 10\n",
    "WARM_UP_STEP = 100\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min([os.cpu_count(), BATCH_SIZE if BATCH_SIZE > 1 else 0, 8])\n",
    "\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "FFN_DIM = 2048\n",
    "MAX_SEQ_LEN = 256\n",
    "DROP_PROB = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pervinco/Datasets/Multi30k is already exist.\n",
      "/home/pervinco/Datasets/Multi30k/cache is already exist.\n"
     ]
    }
   ],
   "source": [
    "download_multi30k(DATA_DIR)\n",
    "make_cache(f\"{DATA_DIR}/Multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6274 8041\n"
     ]
    }
   ],
   "source": [
    "DATASET = Multi30kDataset(data_dir=f\"{DATA_DIR}/Multi30k\", source_language=SRC_LANGUAGE,  target_language=TGT_LANGUAGE,  max_seq_len=MAX_SEQ_LEN, vocab_min_freq=2)\n",
    "\n",
    "train_iter, valid_iter, test_iter = DATASET.get_iter(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "src_vocab_size, trg_vocab_size = len(DATASET.src_vocab), len(DATASET.trg_vocab)\n",
    "print(src_vocab_size, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15]) torch.Size([14])\n",
      "tensor([  2,   6,  12,   7,   4, 387,  24,  10, 268,  11,  38,   8, 123,   5,\n",
      "          3])\n",
      "tensor([   2,    5,   12,    7,    6,  422,   43,  216,    9,   42, 1534,  136,\n",
      "           4,    3])\n"
     ]
    }
   ],
   "source": [
    "src_batch, tgt_batch = next(iter(train_iter))\n",
    "src_sample, tgt_sample = src_batch[0], tgt_batch[0]\n",
    "\n",
    "print(src_sample.shape, tgt_sample.shape)\n",
    "print(src_sample)\n",
    "print(tgt_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Embedding, Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * np.sqrt(x.size(-1))\n",
    "        x = self.positional_encoding(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 512])\n"
     ]
    }
   ],
   "source": [
    "## 11개의 토큰이 512 차원의 벡터로 임베딩.\n",
    "\n",
    "src_embedder = EmbeddingLayer(src_vocab_size, D_MODEL)\n",
    "src_embed = src_embedder(src_sample.unsqueeze(0))\n",
    "\n",
    "print(src_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 512])\n"
     ]
    }
   ],
   "source": [
    "trg_embedder = EmbeddingLayer(trg_vocab_size, D_MODEL)\n",
    "trg_embed = trg_embedder(tgt_sample.unsqueeze(0))\n",
    "\n",
    "print(trg_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(query, key, pad_idx=1):\n",
    "    \"\"\"\n",
    "    Padding Mask\n",
    "        query: (n_batch, query_seq_len)\n",
    "        key: (n_batch, key_seq_len)\n",
    "    \"\"\"\n",
    "    query_seq_len, key_seq_len = query.size(1), key.size(1) ## 소스 문장과 타겟 문장의 길이\n",
    "\n",
    "    ## key.ne(pad_idx)는 key 시퀀스에서 패딩 토큰(pad_idx)이 아닌 위치를 True로 표시하는 마스크를 생성.\n",
    "    key_mask = key.ne(pad_idx).unsqueeze(1).unsqueeze(2)  # (n_batch, 1, 1, key_seq_len)\n",
    "\n",
    "    ## key_mask.repeat(1, 1, query_seq_len, 1)은 query 시퀀스의 길이만큼 key_mask를 복제하여 크기를 (n_batch, 1, query_seq_len, key_seq_len)으로 만든다.\n",
    "    key_mask = key_mask.repeat(1, 1, query_seq_len, 1)    # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "    ## query.ne(pad_idx)는 query 시퀀스에서 패딩 토큰이 아닌 위치를 True로 표시하는 마스크를 생성\n",
    "    query_mask = query.ne(pad_idx).unsqueeze(1).unsqueeze(3)  # (n_batch, 1, query_seq_len, 1)\n",
    "\n",
    "    ## query_mask.repeat(1, 1, 1, key_seq_len)은 key 시퀀스의 길이만큼 query_mask를 복제하여 크기를 (n_batch, 1, query_seq_len, key_seq_len)으로 만든다.\n",
    "    query_mask = query_mask.repeat(1, 1, 1, key_seq_len)  # (n_batch, 1, query_seq_len, key_seq_len)\n",
    "\n",
    "    mask = key_mask & query_mask  # 두 행렬에서 True인 원소만 True로.\n",
    "    mask.requires_grad = False\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def make_subsequent_mask(query, key):\n",
    "    \"\"\"\n",
    "    Look-Ahead Mask\n",
    "        query : (batch_size, query_seq_len)\n",
    "        key : (batch_size, key_seq_len)\n",
    "    \"\"\"\n",
    "    query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
    "\n",
    "    tril = np.tril(np.ones((query_seq_len, key_seq_len)), k=0).astype('uint8')  # lower triangle without diagonal\n",
    "    mask = torch.tensor(tril, dtype=torch.bool, requires_grad=False, device=query.device)  # boolean type의 텐서로 변환.\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def make_src_mask(src):\n",
    "    pad_mask = make_pad_mask(src, src)\n",
    "    return pad_mask\n",
    "\n",
    "def make_tgt_mask(tgt):\n",
    "    pad_mask = make_pad_mask(tgt, tgt)\n",
    "    seq_mask = make_subsequent_mask(tgt, tgt)\n",
    "    mask = pad_mask & seq_mask\n",
    "    return mask\n",
    "\n",
    "def make_src_tgt_mask(src, tgt):\n",
    "    pad_mask = make_pad_mask(tgt, src)\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Padding Mask: torch.Size([1, 1, 15, 15]) \n",
      "tensor([[[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "src_mask = make_src_mask(src_sample.unsqueeze(0))\n",
    "print(f\"Source Padding Mask: {src_mask.shape} \\n{src_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_x = tgt_sample.unsqueeze(0)[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Mask: torch.Size([1, 1, 13, 13]) \n",
      "tensor([[[[ True, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True, False, False, False, False, False, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True, False, False, False, False, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True,  True, False],\n",
      "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "tgt_mask = make_tgt_mask(trg_x)\n",
    "print(f\"Target Mask: {tgt_mask.shape} \\n{tgt_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source-Target Mask: torch.Size([1, 1, 13, 15]) \n",
      "tensor([[[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "src_tgt_mask = make_src_tgt_mask(src_sample.unsqueeze(0), trg_x)\n",
    "print(f\"Source-Target Mask: {src_tgt_mask.shape} \\n{src_tgt_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
