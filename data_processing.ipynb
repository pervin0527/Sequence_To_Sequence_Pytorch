{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"flickr8k\"\n",
    "data_dir = \"/home/pervinco/Datasets\"\n",
    "\n",
    "captions_per_image = 5\n",
    "min_word_frequency = 5\n",
    "max_sequence_length = 50\n",
    "output_dir = f\"{data_dir}/ImageCaption_Dataset\"\n",
    "image_dir = f\"{data_dir}/Flickr8k_dataset/Images\"\n",
    "karpathy_caption_data_path = f\"{data_dir}/karpathy_caption_datasets/dataset_{dataset_name}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr8k\n",
      "8000\n",
      "{'sentids': [0, 1, 2, 3, 4], 'imgid': 0, 'sentences': [{'tokens': ['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], 'raw': 'A black dog is running after a white dog in the snow .', 'imgid': 0, 'sentid': 0}, {'tokens': ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], 'raw': 'Black dog chasing brown dog through snow', 'imgid': 0, 'sentid': 1}, {'tokens': ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], 'raw': 'Two dogs chase each other across the snowy ground .', 'imgid': 0, 'sentid': 2}, {'tokens': ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], 'raw': 'Two dogs play together in the snow .', 'imgid': 0, 'sentid': 3}, {'tokens': ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water'], 'raw': 'Two dogs running through a low lying body of water .', 'imgid': 0, 'sentid': 4}], 'split': 'train', 'filename': '2513260012_03d33305cf.jpg'}\n",
      "{'sentids': [5, 6, 7, 8, 9], 'imgid': 1, 'sentences': [{'tokens': ['a', 'little', 'baby', 'plays', 'croquet'], 'raw': 'A little baby plays croquet .', 'imgid': 1, 'sentid': 5}, {'tokens': ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], 'raw': 'A little girl plays croquet next to a truck .', 'imgid': 1, 'sentid': 6}, {'tokens': ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], 'raw': 'The child is playing croquette by the truck .', 'imgid': 1, 'sentid': 7}, {'tokens': ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball'], 'raw': 'The kid is in front of a car with a put and a ball .', 'imgid': 1, 'sentid': 8}, {'tokens': ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car'], 'raw': 'The little boy is playing with a croquet hammer and ball beside the car .', 'imgid': 1, 'sentid': 9}], 'split': 'train', 'filename': '2903617548_d3e38d7f88.jpg'}\n",
      "\n",
      "sentids [0, 1, 2, 3, 4]\n",
      "imgid 0\n",
      "sentences [{'tokens': ['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], 'raw': 'A black dog is running after a white dog in the snow .', 'imgid': 0, 'sentid': 0}, {'tokens': ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], 'raw': 'Black dog chasing brown dog through snow', 'imgid': 0, 'sentid': 1}, {'tokens': ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], 'raw': 'Two dogs chase each other across the snowy ground .', 'imgid': 0, 'sentid': 2}, {'tokens': ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], 'raw': 'Two dogs play together in the snow .', 'imgid': 0, 'sentid': 3}, {'tokens': ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water'], 'raw': 'Two dogs running through a low lying body of water .', 'imgid': 0, 'sentid': 4}]\n",
      "{'tokens': ['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], 'raw': 'A black dog is running after a white dog in the snow .', 'imgid': 0, 'sentid': 0}\n",
      "{'tokens': ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], 'raw': 'Black dog chasing brown dog through snow', 'imgid': 0, 'sentid': 1}\n",
      "{'tokens': ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], 'raw': 'Two dogs chase each other across the snowy ground .', 'imgid': 0, 'sentid': 2}\n",
      "{'tokens': ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], 'raw': 'Two dogs play together in the snow .', 'imgid': 0, 'sentid': 3}\n",
      "{'tokens': ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water'], 'raw': 'Two dogs running through a low lying body of water .', 'imgid': 0, 'sentid': 4}\n",
      "split train\n",
      "filename 2513260012_03d33305cf.jpg\n"
     ]
    }
   ],
   "source": [
    "with open(karpathy_caption_data_path, 'r') as f:\n",
    "    data = json.load(f) ## images, dataset\n",
    "\n",
    "print(data['dataset'])\n",
    "print(len(data['images']))\n",
    "\n",
    "print(data['images'][0])\n",
    "print(data['images'][1])\n",
    "print()\n",
    "\n",
    "for key, value in data['images'][0].items():\n",
    "    print(key, value)\n",
    "\n",
    "    if key == 'sentences':\n",
    "        for sentence in value:\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_files(ds_name, json_path, img_ds_path, captions_per_image, min_word_freq, output_path, max_seq_len=100):\n",
    "    assert ds_name in {\"coco\", \"flickr8k\", \"flickr30k\"}\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    with open(json_path, 'r') as j:\n",
    "        data = json.load(j)\n",
    "\n",
    "    word_freq = Counter()\n",
    "    train_images, train_captions = [], []\n",
    "    valid_images, valid_captions = [], []\n",
    "    test_images, test_captions = [], []\n",
    "\n",
    "    for img in data['images']:\n",
    "        captions = []\n",
    "        for c in img['sentences']: ## 이미지 파일마다 여러 개의 caption(sentence)이 존재.\n",
    "            word_freq.update(c['tokens']) ## 문장마다 token list가 있음. 이것을 통해 word_frequencies를 계산.\n",
    "\n",
    "            if len(c['tokens']) <= max_seq_len:\n",
    "                captions.append(c['tokens']) ## 시퀀스 최대 길이 미만인 문장만 사용.\n",
    "\n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "\n",
    "        if ds_name == \"coco\":\n",
    "            path = os.path.join(img_ds_path, img['filepath'], img['filename'])\n",
    "        else:\n",
    "            path = os.path.join(img_ds_path, img['filename'])\n",
    "\n",
    "        if img['split'] in {'train', 'restval'}:\n",
    "            train_images.append(path)\n",
    "            train_captions.append(captions)\n",
    "        elif img['split'] in {'val'}:\n",
    "            valid_images.append(path)\n",
    "            valid_captions.append(captions)\n",
    "        elif img['split'] in {'test'}:\n",
    "            test_images.append(path)\n",
    "            test_captions.append(captions)\n",
    "\n",
    "    assert len(train_images) == len(train_captions)\n",
    "    assert len(valid_images) == len(valid_captions)\n",
    "    assert len(test_images) == len(test_captions)\n",
    "\n",
    "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "    word_map = {k : v + 1 for v, k in enumerate(words)}\n",
    "    word_map['<unk>'] = len(word_map) + 1\n",
    "    word_map['<sos>'] = len(word_map) + 1\n",
    "    word_map['<eos>'] = len(word_map) + 1\n",
    "    word_map['<pad>'] = 0\n",
    "\n",
    "    random.seed(123)\n",
    "    base_filename = ds_name + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "    with open(os.path.join(output_path, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
    "        json.dump(word_map, j)\n",
    "\n",
    "    for impaths, imcaps, split in [(train_images, train_captions, 'TRAIN'), (valid_images, valid_captions, 'VAL'), (test_images, test_captions, 'TEST')]:\n",
    "        with h5py.File(os.path.join(output_path, split + \"_IMAGES_\" + base_filename + \".hdf5\"), 'a') as h:\n",
    "            h.attrs[\"captions_per_image\"] = captions_per_image\n",
    "            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
    "            print(\"Reading %s images and captions, storing to file...\\n\" % split)\n",
    "\n",
    "            caplens = []\n",
    "            enc_captions = []\n",
    "            for i, path in enumerate(tqdm(impaths)):\n",
    "                if len(imcaps[i]) < captions_per_image:\n",
    "                    captions = imcaps[i] + [random.choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "                else:\n",
    "                    captions = random.sample(imcaps[i], k=captions_per_image)\n",
    "\n",
    "                assert len(captions) == captions_per_image\n",
    "\n",
    "                img = cv2.imread(impaths[i])\n",
    "                if len(img.shape) == 2:\n",
    "                    img = img[:, :, np.newaxis]\n",
    "                    img = np.concatenate([img, img, img], axis=2)\n",
    "                    \n",
    "                img = cv2.resize(img, (256, 256))\n",
    "                img = img.transpose(2, 0, 1)\n",
    "                assert img.shape == (3, 256, 256)\n",
    "                assert np.max(img) <= 255\n",
    "\n",
    "\n",
    "                images[i] = img\n",
    "                for j, c in enumerate(captions):\n",
    "                    # Encode captions\n",
    "                    enc_c = [word_map['<sos>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [word_map['<eos>']] + [word_map['<pad>']] * (max_seq_len - len(c))\n",
    "\n",
    "                    # Find caption lengths\n",
    "                    c_len = len(c) + 2\n",
    "\n",
    "                    enc_captions.append(enc_c)\n",
    "                    caplens.append(c_len)\n",
    "\n",
    "            # Sanity check\n",
    "            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
    "\n",
    "            # Save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(output_path, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(enc_captions, j)\n",
    "\n",
    "            with open(os.path.join(output_path, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(caplens, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TRAIN images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:12<00:00, 476.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading VAL images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 460.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TEST images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 460.19it/s]\n"
     ]
    }
   ],
   "source": [
    "create_input_files(ds_name=dataset_name,\n",
    "                   json_path=karpathy_caption_data_path,\n",
    "                   img_ds_path=image_dir,\n",
    "                   captions_per_image=captions_per_image,\n",
    "                   min_word_freq=min_word_frequency,\n",
    "                   output_path=output_dir,\n",
    "                   max_seq_len=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
